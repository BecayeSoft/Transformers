{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "To create an initial attention matrix, we need every work to look at every other workds in order to compare the affinities between them. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { Attention }(Q, K, V) &=\\operatorname{softmax} \\left(\\frac{Q. K^{T}} {\\sqrt{d_{k}}} + M \\right) V \\\\\n",
    "\\text { MultiHead }(Q, K, V) &=\\text { Concat }\\left(\\text {head}_{1}, \\ldots, \\text { head }_{h}\\right) W^{O} \\\\\n",
    "\\text { where head }_{i} &=\\text { Attention }\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Q (Query) is what we are looking for (a matrix of words we want to find the affinities for)\n",
    "- K (Key) is what we got. It is like a descriptor or identifier for each element in the sequence (a matrix of words we want to compare the query to)\n",
    "- V (Value): is the answer, the actual words that we want to return.. \n",
    "    \n",
    "For example, in the sentence: \"The animal didn't cross the street because it was too tired\", the word \"it\" is a pronoun that refers to the word \"street\". Therefore, the word \"street\" is the value.\t\n",
    "     E.g.: \"cat\" is the query, \"feline\" is the key, and \"cat\" is the value\n",
    "- M represents the mask (hide future words to avoid cheating or data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  [[ 0.07232786 -0.59195325 -0.1419609   0.77552694  0.44124696 -1.97112574\n",
      "  -0.38431624 -1.34772469]\n",
      " [-0.75082145  0.05107283  1.08104267 -0.10403621  0.22744124  0.99602557\n",
      "   1.4203686  -0.48494464]\n",
      " [ 1.1651399   1.03615217 -0.41530289 -2.17987454 -1.08842478  0.78854814\n",
      "  -0.4572014  -2.04668457]\n",
      " [ 0.10619677 -0.41690603 -0.63991237  0.3152003  -0.53981453  1.12299832\n",
      "   0.17310038  1.02869691]]\n",
      "\n",
      "K:  [[-0.95490556 -0.24524066  2.4711064  -0.50403096 -0.29284632  0.16942138\n",
      "   0.77620133 -0.10588287]\n",
      " [-0.4312431  -1.0853785  -0.32632753 -0.60694835 -0.05643407 -0.06701821\n",
      "  -0.60725339  0.32265753]\n",
      " [-0.31492119 -1.05385367 -0.68237752 -1.78838182 -0.37308738  1.01633119\n",
      "  -0.51998175 -0.69869263]\n",
      " [ 1.45830128  0.21022749 -0.20899192 -0.10520092 -0.10807048 -0.43582661\n",
      "  -0.86607773 -1.02268841]]\n",
      "\n",
      "V:  [[ 0.24345222  0.18000543 -0.70356586 -0.58213488  0.1887324   0.18413888\n",
      "  -0.72599888 -1.00914211]\n",
      " [ 0.7336737  -0.7366185  -0.79201003 -1.66652835  1.09357679  1.2101871\n",
      "   1.19436486 -0.1395625 ]\n",
      " [ 0.27284825  1.40833987  0.25383996  2.32870208  1.02916032  0.74427341\n",
      "  -0.87619586 -0.19458784]\n",
      " [ 1.12859828 -0.37376461 -0.46840821 -0.71206725 -2.0852268   0.92654678\n",
      "  -0.85635522  1.66687045]]\n"
     ]
    }
   ],
   "source": [
    "# Length of the input sequence\n",
    "# The input is a sequence of 4 words\n",
    "seq_length = 4\n",
    "\n",
    "# Dimension of the embedding space, i\n",
    "# Each world will be represented as a vector of size 8\n",
    "embedding_dim = 8\n",
    "\n",
    "Q = np.random.randn(seq_length, embedding_dim)\n",
    "K = np.random.randn(seq_length, embedding_dim)\n",
    "V = np.random.randn(seq_length, embedding_dim)\n",
    "\n",
    "print(\"Q: \", Q)\n",
    "print(\"\\nK: \", K)\n",
    "print(\"\\nV: \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing (Q @ K.T) the product by the square root of the dim reduces the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.87233951474035, 0.7340424393425435)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Q @ K.T).var(), ((Q @ K.T) / np.sqrt(embedding_dim)).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled-dot product attention\n",
    "\n",
    "We are going to compute the scaled dot product attention.\n",
    "\n",
    "![Alt text](imgs/multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matmul: Q @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.28435957,  0.09264717, -1.7154703 ,  2.45164693],\n",
       "       [ 4.68422941, -1.11985814,  0.15870145, -2.49204791],\n",
       "       [-0.98007198, -0.54264375,  5.59820479,  4.49612129],\n",
       "       [-1.36554585,  0.60621855,  0.81286187, -1.46525039]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul = Q @ K.T\n",
    "matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = matmul / np.sqrt(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "The mask will just mask the next words as we will see below.\n",
    "\n",
    "For the decoder, in reality, we aren't supposed to know the next word. So looking at the next words when trying to generate the context of the current word is cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. -inf -inf -inf]\n",
      " [  0.   0. -inf -inf]\n",
      " [  0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.]] \n",
      "\n",
      "[['My' '' '' '']\n",
      " ['My' 'Name' '' '']\n",
      " ['My' 'Name' 'is' '']\n",
      " ['My' 'Name' 'is' 'Becaye']]\n"
     ]
    }
   ],
   "source": [
    "# Creating the mask\n",
    "mask = np.tril(np.ones((seq_length, seq_length)))\n",
    "mask[mask == 0] = -np.inf\n",
    "mask[mask == 1] = 0\n",
    "print(mask,  \"\\n\")\n",
    "\n",
    "# To get a more intuitive understanding \n",
    "# of the mask, let's fill it with words\n",
    "mask_words = np.full_like(mask, fill_value='', dtype=object)\n",
    "mask_words[..., 0] = \"My\"\n",
    "mask_words[1:, 1] = \"Name\"\n",
    "mask_words[2:, 2] = \"is\"\n",
    "mask_words[3:, 3] = \"Becaye\"\n",
    "print(mask_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying the maskk to the attention matrix**\n",
    "\n",
    "This will make the softmax ignore the masked values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Scale -------\n",
      "[[-0.45408968  0.03275572 -0.60651034  0.86678808]\n",
      " [ 1.65612519 -0.39592964  0.05610943 -0.88107199]\n",
      " [-0.34650777 -0.19185354  1.97926428  1.58961893]\n",
      " [-0.48279337  0.21433062  0.28739007 -0.51804424]]\n",
      "\n",
      "------- Scale + Mask -------\n",
      "[[-0.45408968        -inf        -inf        -inf]\n",
      " [ 1.65612519 -0.39592964        -inf        -inf]\n",
      " [-0.34650777 -0.19185354  1.97926428        -inf]\n",
      " [-0.48279337  0.21433062  0.28739007 -0.51804424]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n------- Scale -------')\n",
    "print(scale)\n",
    "\n",
    "# Applying the mask will only hide the future words\n",
    "print('\\n------- Scale + Mask -------')\n",
    "print(scale + mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { Softmax }(x)_{i} &=\\frac{\\exp \\left(x_{i}\\right)}{\\sum_{j} \\exp \\left(x_{j}\\right)} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fn(x):\n",
    "    # Compute the exponential of each element in x\n",
    "    exp_x = np.exp(x)\n",
    "    \n",
    "    # Sum the exponentials along the last dimension (axis=-1)\n",
    "    sum_exp_x = np.sum(exp_x, axis=-1, keepdims=True)  # Keepdims to maintain shape\n",
    "    \n",
    "    # Compute the softmax values by dividing each element of exp_x by sum_exp_x\n",
    "    softmax_x = exp_x / sum_exp_x\n",
    "    \n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the mask, we can see that the attention also focuses on the next words. But with the mask, only the current words are focused on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Without Mask -------\n",
      "[[0.13826457 0.22498066 0.1187177  0.51803707]\n",
      " [0.70949574 0.09114938 0.14324246 0.05611243]\n",
      " [0.0517232  0.06037413 0.52936519 0.35853747]\n",
      " [0.16303918 0.32737769 0.35219111 0.15739202]]\n",
      "\n",
      "------- With Mask -------\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.88615508 0.11384492 0.         0.        ]\n",
      " [0.08063324 0.0941195  0.82524726 0.        ]\n",
      " [0.16303918 0.32737769 0.35219111 0.15739202]]\n"
     ]
    }
   ],
   "source": [
    "# Without mask\n",
    "softmax = softmax_fn(scale)\n",
    "print(\"------- Without Mask -------\")\n",
    "print(softmax)\n",
    "\n",
    "\n",
    "softmax = softmax_fn(scale + mask)\n",
    "print(\"\\n------- With Mask -------\")\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Before Attention -------\n",
      "[[ 0.24345222  0.18000543 -0.70356586 -0.58213488  0.1887324   0.18413888\n",
      "  -0.72599888 -1.00914211]\n",
      " [ 0.7336737  -0.7366185  -0.79201003 -1.66652835  1.09357679  1.2101871\n",
      "   1.19436486 -0.1395625 ]\n",
      " [ 0.27284825  1.40833987  0.25383996  2.32870208  1.02916032  0.74427341\n",
      "  -0.87619586 -0.19458784]\n",
      " [ 1.12859828 -0.37376461 -0.46840821 -0.71206725 -2.0852268   0.92654678\n",
      "  -0.85635522  1.66687045]]\n",
      "\n",
      "------- After Attention -------\n",
      "[[ 0.24345222  0.18000543 -0.70356586 -0.58213488  0.1887324   0.18413888\n",
      "  -0.72599888 -1.00914211]\n",
      " [ 0.29926144  0.07565246 -0.71363477 -0.70558756  0.29174433  0.30094926\n",
      "  -0.50737523 -0.91014489]\n",
      " [ 0.31385061  1.10741288  0.07820635  1.71796277  0.96745674  0.74295951\n",
      "  -0.66920485 -0.25508903]\n",
      " [ 0.55360774  0.22537269 -0.35831875  0.06757948  0.42304647  0.83416766\n",
      "  -0.17072973 -0.01639935]]\n"
     ]
    }
   ],
   "source": [
    "# attention_V = softmax_fn(scale + mask) @ V\n",
    "attention_V = softmax @ V\n",
    "\n",
    "print(\"\\n------- Before Attention -------\")\n",
    "print(V)\n",
    "\n",
    "print(\"\\n------- After Attention -------\")\n",
    "print(attention_V)\n",
    "\n",
    "# Applying the attention has modified the values of the vectors\n",
    "# to better encapsulate the context of the workd.\n",
    "# Notice how the first has remained intact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
