{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the attention mechanism work?\n",
    "\n",
    "*In this example, we focus on Natural Language Processing*\n",
    "\n",
    "The attention mechanism allows the model to focus on relevant parts of the input data.\n",
    "It works as follows:\n",
    "\n",
    "1. **Encoding**\n",
    "\n",
    "The model receives the input data (e.g. a sentence) and encodes it into a vector representation (embedding).\n",
    "\n",
    "2. **Query, Key, Value**\n",
    "\n",
    "The attention mechanism relies on these 3 types of vectors. They are obtained by multiplying the 3 weights matrices (Query, Key, Value) by the encoded input data.\n",
    "\n",
    "3. **Attention scores**\n",
    "\n",
    "The scores are obtained by multiplying the Query vector of a single word by the Key vector of all the other words. This gives us the similarity between the word and all the other words.\n",
    "\n",
    "4. **Softmax**\n",
    "\n",
    "The softmax function maps the scores to probability making them easily interpretable.\n",
    "\n",
    "5. **Weighted sum**\n",
    "\n",
    "The weighted sum is obtained by multiplying the softmax scores by the Value vectors.\n",
    "\n",
    "*The weighted sum is the output, commonly referred to as the \"Context Vector\".*\n",
    "\n",
    "## The General Attention Mechanism\n",
    "\n",
    "![translation-with-attention](imgs/translation-with-attention.png)\n",
    "Source: [cloudskillsboost.google](https://www.cloudskillsboost.google/course_sessions/3962322/video/383836)\n",
    "\n",
    "- ${H}$ is the encoder hidden state at each time step. In other words, the Key vector.\n",
    "- ${H}(d)$ is the decoder hidden state at each time step. In other words, the Query vector.\n",
    "- ${a}$ is the attention weights at each time step. In other words, the attention scores.\n",
    "\n",
    "### Value, Key, Query\n",
    "\n",
    "**Value vectors**\n",
    "\n",
    "The Value represent the actual input data. The value vector is the embeddings of each word in the input sentence.\n",
    "\n",
    "**Key vectors - ${H}$**\n",
    "\n",
    "They Key is the encoded representation of the input data. Each hidden state generated by the encoder becomes a Key vector.\n",
    "\n",
    "**Query vectors - ${H}(d)$**\n",
    "\n",
    "The Query vector is the output of the decoder at each decoding step. \n",
    "\n",
    "Let's say we want to translate \"I love cats\" in french:\n",
    "- The Value vectors would be the embeddings of the source sentence. The embeddings of \"I\", \"love\" and \"cats\".\n",
    "- The Key vector would be the encoded sentence, which is nothing other than the hidden states of the encoder. The hidden states of \"I\", \"love\" and \"cats\". \n",
    "- The query vector could be the translated word. In our case, it could be the embedding of \"J'aime\".\n",
    "\n",
    "### Attention Scores - ${a}$\n",
    "\n",
    "**Score Formula**:\n",
    "\n",
    "The attention score between a query vector $Q$ and a key vector $K$ can be obtained using the dot product. Given $Q$ as an $m$-dimensional vector and $K$ as an $n$-dimensional vector, the score (similarity) can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{Score}(Q, K) = Q \\cdot K^T\n",
    "$$\n",
    "\n",
    "Mathematical notation:\n",
    "$$\n",
    "\\text{e}(q, k_i) = q * k_i\n",
    "$$\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "$$\n",
    "\\text{softmax}(S) = \\frac{\\exp(S)}{\\sum{\\exp(S)}}\n",
    "$$\n",
    "\n",
    "**Weighted sum (Context vector)**\n",
    "\n",
    "$$\n",
    "C = \\text{softmax}(S) \\cdot V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh on matrix multiplication\n",
    "\n",
    "Before diving into the attention mechanism, let's refresh our memory on matrix multiplication.\n",
    "\n",
    "- M1 * M2 is the element-wise product of Matrice 1 by Matrice 2\n",
    "```python\n",
    "[1, 2] * [5, 6] = [5, 12]\n",
    "[3, 4]   [7, 8]   [21, 32]\n",
    "```\n",
    "\n",
    "- M1 @ M2 is the multiplication of Matrice 1 by Matrice 2\n",
    "```python\n",
    "        [5, 6]\n",
    "        [7, 8]\n",
    "[1, 2] @       =  [19, 22]\n",
    "[3, 4]            [43, 50]\n",
    "```\n",
    "\n",
    "- np.dot(M1, M2) is the same as M1 @ M2 but is compatible with more types of objects.\n",
    "\n",
    "In pratice, we often need to multiply matrices all at once, instead of taking each of them one by one:\n",
    "\n",
    "```python\n",
    "Q = [2 0 2]        K = [2 2 2]\n",
    "    [2 0 0]            [0 2 1]\n",
    "    [4 0 2]            [2 4 3]\n",
    "    [2 1 2]            [0 1 1]\n",
    "\n",
    "# We want to multiply each Query by all the Keys (E.g. Q[0] * K)\n",
    "\n",
    "We could use:\n",
    "scores[0] = Q[0] @ K[0], Q[0] @ K[1], ...\n",
    "scores[1] = Q[1] @ K[0], Q[1] @ K[1], ...\n",
    "\n",
    "print(scores[0])\n",
    "print(scores[1])\n",
    "\n",
    "... [8 4 8 2]\n",
    "... [4 0 4 0]\n",
    "```\n",
    "\n",
    "But a more convenient way to do so is to transpose the Keys matrix and multiply it by the Query matrix:\n",
    "\n",
    "```python\n",
    "# We transpose K\n",
    "K.T = [2 0 2 0]\n",
    "      [2 2 4 1]\n",
    "      [2 1 3 1]\n",
    "\n",
    "# We compute Q @ K.T\n",
    "\n",
    "                [[2 0 2 0]\n",
    "                [2 2 4 1]\n",
    "                [2 1 3 1]]\n",
    "            @\n",
    " [2 0 2]        [8 4 8 2]\n",
    " [2 0 0]    =   [4 0 4 0]\n",
    " [4 0 2]        [8 4 8 2]\n",
    " [2 1 2]        [6 2 6 2]               \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "In this example, we have four words as input, the goal is to calculate their attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "\n",
    "We are going to generate some fake data:\n",
    "1. Embeddings of 4 words generated by the encoder\n",
    "2. Weights matrices of Query, Key, Value for each of the 4 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Generate the encoder representations (embeddings) of the words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0]), array([0, 1, 0]), array([1, 1, 0]), array([0, 0, 1]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = np.array([1, 0, 0])\n",
    "word_2 = np.array([0, 1, 0])\n",
    "word_3 = np.array([1, 1, 0])\n",
    "word_4 = np.array([0, 0, 1])\n",
    "\n",
    "word_1, word_2, word_3, word_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Generate the weights matrices**\n",
    "\n",
    "These weights will be multiplied by the word embeddings to generate the Queries, Keys and Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 0, 2],\n",
       "        [2, 0, 0],\n",
       "        [2, 1, 2]]),\n",
       " array([[2, 2, 2],\n",
       "        [0, 2, 1],\n",
       "        [0, 1, 1]]),\n",
       " array([[1, 1, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 0, 0]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q = np.random.randint(3, size=(3, 3))\n",
    "W_K = np.random.randint(3, size=(3, 3))\n",
    "W_V = np.random.randint(3, size=(3, 3))\n",
    "\n",
    "W_Q, W_K, W_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Attention\n",
    "\n",
    "1. First, we calculate the Query, Key, Value for each word:\n",
    "\n",
    "    - queries[i] = words[i] @ W_Query\n",
    "    - keys[i] = words[i] @ W_Keys\n",
    "    - values[i] = words[i] @ W_Values\n",
    "\n",
    "2. Next, we score each word by taking the dot product of its query againts all the keys:\n",
    "\n",
    "    - scores[i] = [ np.dot(queries[i] * keys[0]), np.dot(queries[i] * keys[1]), ... ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the Query, Key and Value for each word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 2]), array([2, 2, 2]), array([1, 1, 0]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1 = word_1 @ W_Q\n",
    "key_1 = word_1 @ W_K\n",
    "value_1 = word_1 @ W_V\n",
    "\n",
    "query_2 = word_2 @ W_Q\n",
    "key_2 = word_2 @ W_K\n",
    "value_2 = word_2 @ W_V\n",
    "\n",
    "query_3 = word_3 @ W_Q\n",
    "key_3 = word_3 @ W_K\n",
    "value_3 = word_3 @ W_V\n",
    "\n",
    "query_4 = word_4 @ W_Q\n",
    "key_4 = word_4 @ W_K\n",
    "value_4 = word_4 @ W_V\n",
    "\n",
    "query_1, key_1, value_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score Values**\n",
    "\n",
    "Now, considering each word,we score its query vector againts all the key vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 = np.array([[1, 2], [3, 4]])\n",
    "matrix2 = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "matrix1 @ matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  2, 10,  2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring the first query vector against all vectors\n",
    "scores = query_1 @ key_1, query_1 @ key_2, query_1 @ key_3, query_1 @ key_4\n",
    "scores = np.array(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply softmax to the scores**\n",
    "\n",
    "The softmax function transform the attention scores to probabilities making it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23608986, 0.00738988, 0.74913039, 0.00738988])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To keep the gradient stable, we divide the scores by the square root of the dimension of the key vectors\n",
    "d = np.sqrt(key_1.shape[0])\n",
    "weights = softmax(scores / d)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the weighted sum of weights and values**\n",
    "\n",
    "Now, we use the attention weights to get a weighted sum of the Value vector. The weighted sum gives more importance to values that have higher attention weights.\n",
    "This allow the model to focus on relevant parts of the input data.\n",
    "\n",
    "*The weighted sum is nothing other than the context vector, whichi  represents aggregated information from the input data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98522025, 1.74174051, 0.75652026])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! This  context vectors will be used as an input to the subsequent layers of the Deep Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now, let's put all the pieces together to build a complete attention model for all the 4 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98522025 1.74174051 0.75652026]\n",
      " [0.90965265 1.40965265 0.5       ]\n",
      " [0.99851226 1.75849334 0.75998108]\n",
      " [0.99560386 1.90407309 0.90846923]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import random\n",
    "from numpy import dot\n",
    "from numpy.random import randint\n",
    "from scipy.special import softmax\n",
    "np.random.seed(42)\n",
    "\n",
    "# encoder representations of four different words\n",
    "word_1 = array([1, 0, 0])\n",
    "word_2 = array([0, 1, 0])\n",
    "word_3 = array([1, 1, 0])\n",
    "word_4 = array([0, 0, 1])\n",
    "\n",
    "# stacking the word embeddings into a single array\n",
    "words = array([word_1, word_2, word_3, word_4])\n",
    "\n",
    "# generating the weight matrices\n",
    "W_Q = randint(3, size=(3, 3))\n",
    "W_K = randint(3, size=(3, 3))\n",
    "W_V = randint(3, size=(3, 3))\n",
    "\n",
    "# generating the queries, keys and values\n",
    "Q = words @ W_Q\n",
    "K = words @ W_K\n",
    "V = words @ W_V\n",
    "\n",
    "# scoring each query vector against all key vectors\n",
    "scores = Q @ K.T\n",
    "\n",
    "# computing the weights by a softmax operation\n",
    "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
    "\n",
    "# computing the attention by a weighted sum of the value vectors\n",
    "attention = weights @ V\n",
    "\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://machinelearningmastery.com/the-attention-mechanism-from-scratch/\n",
    "- https://www.cloudskillsboost.google/course_sessions/3962322/quizzes/383837\n",
    "\n",
    "\n",
    "- The following prompts were used with ChatGPT\n",
    "    - How does the attention mechanism work?\n",
    "    - Is the weighted sum the context vector?\n",
    "    - Concretely what represent the Query, Value and Key vectors?\n",
    "\n",
    "Answers were carefully reviewed and edited for clarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
