⚠️ Still under development

# Transformers

In this repository, I implement the original Transformer model from the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) in PyTorch.

**TODO**

- [X] Self Attention 
- [X] Multi-head attention
- [X] Positional encoding
- [X] Layer Normalization
- [ ] Encoder
- [ ] Decoder

## References

- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Self Attention in Transformer Neural Networks (with Code!)](https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=1) by CodeEmporium on YouTube
- [Multi Head Attention in Transformer Neural Networks with Code!](https://www.youtube.com/watch?v=HQn1QKQYXVg&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=2) by CodeEmporium on YouTube
- [Positional Encoding in Transformer Neural Networks Explained](https://www.youtube.com/watch?v=ZMxVe-HK174&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=3) by CodeEmporium on YouTube
- [Layer Normalization - EXPLAINED (in Transformer Neural Networks)](https://www.youtube.com/watch?v=G45TuC6zRf4&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=4) by CodeEmporium on YouTube
- [Transformers from Scratch by CodeEmporium on YouTube](https://www.youtube.com/watch?v=G45TuC6zRf4&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=4)
